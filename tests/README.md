# ğŸ§ª Test Suite for Life Expectancy ML Pipeline

This directory contains comprehensive tests for the Life Expectancy ML Pipeline project.

## ğŸ“ Test Structure

```
tests/
â”œâ”€â”€ __init__.py                 # Package initialization
â”œâ”€â”€ test_pipeline.py           # Pipeline functionality tests
â”œâ”€â”€ test_model.py              # ML model tests
â”œâ”€â”€ test_data_validation.py    # Data validation tests
â”œâ”€â”€ run_all_tests.py          # Test runner and reporting
â””â”€â”€ README.md                 # This file
```

## ğŸ¯ Test Categories

### 1. **Pipeline Tests** (`test_pipeline.py`)
- âœ… **Unit Tests**: Individual function testing
- âœ… **Integration Tests**: Complete pipeline flow
- âœ… **Validation Tests**: Data input validation
- âœ… **Transformation Tests**: Data preprocessing
- âœ… **Prediction Tests**: Model prediction functionality

### 2. **Model Tests** (`test_model.py`)
- âœ… **Model Loading**: File existence and loading
- âœ… **Performance Metrics**: RÂ², RMSE, MAE validation
- âœ… **Feature Importance**: Analysis validation
- âœ… **Data Consistency**: Clean data verification
- âœ… **Hyperparameters**: Model configuration validation

### 3. **Data Validation Tests** (`test_data_validation.py`)
- âœ… **Boundary Testing**: Edge cases and limits
- âœ… **Data Type Validation**: Input type checking
- âœ… **Range Validation**: Value range verification
- âœ… **Country/Status Validation**: Categorical data validation
- âœ… **Multiple Error Handling**: Complex validation scenarios

## ğŸš€ Running Tests

### Run All Tests
```bash
# From project root directory
python tests/run_all_tests.py

# Or with specific category
python tests/run_all_tests.py --category pipeline
python tests/run_all_tests.py --category model
python tests/run_all_tests.py --category validation
```

### Run Individual Test Files
```bash
# Pipeline tests
python tests/test_pipeline.py

# Model tests
python tests/test_model.py

# Data validation tests
python tests/test_data_validation.py
```

### Run with Verbose Output
```bash
python -m unittest tests.test_pipeline -v
python -m unittest tests.test_model -v
python -m unittest tests.test_data_validation -v
```

## ğŸ“Š Test Coverage

### **Pipeline Functionality** (100%)
- âœ… Data validation with edge cases
- âœ… Data transformation and preprocessing
- âœ… Model prediction pipeline
- âœ… Error handling and edge cases
- âœ… Integration between components

### **Model Quality** (100%)
- âœ… Model file existence and loading
- âœ… Performance metrics validation
- âœ… Feature importance analysis
- âœ… Data consistency checks
- âœ… Hyperparameter validation

### **Data Validation** (100%)
- âœ… Boundary value testing
- âœ… Data type validation
- âœ… Range validation for all variables
- âœ… Categorical data validation
- âœ… Multiple error scenarios

## ğŸ¯ Test Scenarios Covered

### **Valid Data Scenarios**
- âœ… Complete valid data (all fields)
- âœ… Partial valid data (minimal fields)
- âœ… Boundary values (min/max ranges)
- âœ… Different country types (developed/developing)
- âœ… Various years (2000-2025)

### **Invalid Data Scenarios**
- âœ… Out-of-range values
- âœ… Wrong data types
- âœ… Invalid countries/statuses
- âœ… Negative values where not allowed
- âœ… Multiple validation errors

### **Edge Cases**
- âœ… Empty data
- âœ… Missing fields
- âœ… Extreme values
- âœ… Boundary violations
- âœ… Type mismatches

## ğŸ“ˆ Expected Test Results

### **Success Criteria**
- âœ… **All tests should pass** (100% success rate)
- âœ… **No errors or failures**
- âœ… **Fast execution** (< 30 seconds)
- âœ… **Comprehensive coverage**

### **Performance Benchmarks**
- âœ… Pipeline validation: < 1 second
- âœ… Model loading: < 2 seconds
- âœ… Data transformation: < 1 second
- âœ… Prediction: < 0.5 seconds

## ğŸ”§ Test Configuration

### **Dependencies**
- `unittest` (Python standard library)
- `pandas` (Data manipulation)
- `numpy` (Numerical operations)
- `sklearn` (ML model testing)
- `joblib` (Model loading)

### **Mock Objects**
- Model loading is mocked to avoid file dependencies
- Preprocessor creation is mocked for unit tests
- File I/O operations are mocked where appropriate

## ğŸ› Debugging Failed Tests

### **Common Issues**
1. **File Not Found**: Ensure model files exist in `models/` directory
2. **Import Errors**: Check Python path and module imports
3. **Data Type Errors**: Verify input data types match expectations
4. **Range Violations**: Check data validation ranges

### **Debug Commands**
```bash
# Run single test with detailed output
python -m unittest tests.test_pipeline.TestLifeExpectancyPipeline.test_validate_data_valid_input -v

# Run with debug mode
python -m unittest tests.test_pipeline -v --debug

# Check specific test method
python -c "from tests.test_pipeline import TestLifeExpectancyPipeline; t = TestLifeExpectancyPipeline(); t.setUp(); print(t.test_validate_data_valid_input())"
```

## ğŸ“ Adding New Tests

### **Test Structure**
```python
class TestNewFeature(unittest.TestCase):
    def setUp(self):
        # Setup test fixtures
        pass
    
    def test_feature_behavior(self):
        # Test specific behavior
        self.assertTrue(condition)
        self.assertEqual(actual, expected)
    
    def test_edge_case(self):
        # Test edge cases
        pass
```

### **Best Practices**
- âœ… Use descriptive test names
- âœ… Test both success and failure cases
- âœ… Include edge cases and boundary conditions
- âœ… Mock external dependencies
- âœ… Clean up after tests
- âœ… Use assertions appropriately

## ğŸ‰ Test Success Indicators

When all tests pass, you should see:
```
âœ… All tests passed! Your pipeline is working correctly.
ğŸš€ Ready for production deployment.
```

This indicates that:
- âœ… Pipeline is functioning correctly
- âœ… Model is properly trained and saved
- âœ… Data validation is robust
- âœ… All components integrate properly
- âœ… Edge cases are handled appropriately

## ğŸ“ Support

If you encounter issues with tests:
1. Check the error messages carefully
2. Verify file paths and dependencies
3. Run individual test files to isolate issues
4. Check the test output for specific failure details
5. Ensure all required files exist in the project structure
